#!/usr/bin/env python

import feature
import psycopg2, argparse, getpass, json, sys, zipfile, struct
import multiprocessing, signal, time

def work(args, passwd, chunk):
  # Make sure it is the parent process receives CTRL-C.
  signal.signal(signal.SIGINT, signal.SIG_IGN)

  # Check if the keyboard interrupt is triggered.
  if work.event.is_set():
    return

  try:
    sql = 'host=%s dbname=%s user=%s password=%s' % \
        (args.h, args.d, args.u, passwd)
    conn = psycopg2.connect(sql)
    cur =  conn.cursor()

    for (a_id,) in chunk:
      # Update progress
      with work.counter.get_lock():
        work.counter.value += 1

      # Check if the keyboard interrupt is triggered
      if work.event.is_set():
        raise Exception('Keyboard interrupt')

      try:
        # Get the path of the app.
        sql = 'SELECT a_path FROM apps WHERE a_id = %s'
        cur.execute(sql, (a_id,))
        a_path, = cur.fetchone()
        
        # Get the APIs.
        a_apis = feature.get_apis(a_path)
        
        # Update the column.
        sql = 'UPDATE apps SET a_apis = %s WHERE a_id = %s'
        cur.execute(sql, (a_apis, a_id))
        conn.commit()

      except zipfile.BadZipfile as e:
        pass

      except UnicodeEncodeError as e:
        pass

      except struct.error as e:
        pass

      except Exception as e:
        pass

  except Exception as e:
    pass

  finally:
    cur.close()
    conn.close()


def work_init(counter, event):
  work.counter = counter
  work.event = event


def main():
  #----------------------------------------------------------------------
  # Arguments
  #----------------------------------------------------------------------

  parser = argparse.ArgumentParser(description='Extract APIs from apps \
      and store them to the database', add_help=False)
  parser.add_argument('--help', action='help', help='show this help \
      message.')
  parser.add_argument('-h', metavar='host', type=str, required=True)
  parser.add_argument('-d', metavar='database', type=str, required=True)
  parser.add_argument('-u', metavar='username', type=str, required=True)
  parser.add_argument('-l', metavar='LAN IP of localhost', type=str,
      required=True)
  parser.add_argument('-t', metavar='number of threads', type=int,
      default=4)
  args = parser.parse_args()

  passwd = getpass.getpass()

  #----------------------------------------------------------------------
  # There will be multiple processes running in parallel. We could simply
  # divide the dataset into several groups with the number of which equal
  # to the number of processes. Since, however, the time for decompiling
  # an app is unknown, doing so might cause the workload imbalanced.
  #
  # Therefore, the strategy is divide them into much smaller chunks. Each
  # of processes start from one of chunks, and pick another one when the
  # job is done.
  #----------------------------------------------------------------------
  
  chunks = []
  chunk_size = 100
  total = 0
  
  try:
    sql = 'host=%s dbname=%s user=%s password=%s' % \
        (args.h, args.d, args.u, passwd)
    conn = psycopg2.connect(sql)
    cur =  conn.cursor()

    sql = 'SELECT a_id FROM apps WHERE \
        a_apis IS NULL AND a_ipaddress = %s AND a_file_broken = %s'
    cur.execute(sql, (args.l, False))

    while True:
      chunk = cur.fetchmany(chunk_size)

      # Check if any row is available.
      if chunk == []:
        break

      chunks.append(chunk)
      total += len(chunk)

  except Exception as e:
    print e

  finally:
    cur.close()
    conn.close()

  # For progress status and control
  progress_counter = multiprocessing.Value('i', 0)
  exit_event = multiprocessing.Event()
  children_status = []

  # Create a pool of processes which will carry out tasks submitted.
  pool = multiprocessing.Pool(
      processes=args.t,
      initializer=work_init,
      initargs=(progress_counter, exit_event)
  )

  for chunk in chunks:
    result = pool.apply_async(func=work, args=(args, passwd, chunk))
    children_status.append(result)
  
  # Prevent any tasks from being submitted.
  pool.close()

  # This flag indicates whether all the tasks are finished.
  all_tasks_are_done = False

  while not all_tasks_are_done:
    try:
      # Progress bar
      sys.stdout.write('\r')
      for i in range(80):
        sys.stdout.write(' ')
      sys.stdout.write('\r[%d%%] %d/%d' \
          % (progress_counter.value * 100 / total, progress_counter.value,
            total))
      sys.stdout.flush()

      # Refresh time interval
      time.sleep(4)

      # Check status
      all_tasks_are_done = True
      for result in children_status:
        all_tasks_are_done = all_tasks_are_done and result.ready()

    except KeyboardInterrupt:
      exit_event.set()

      print 'Parent process received a keyboard interrupt.'
      print 'Wait for all the tasks finishing...'

      # Wait for all the tasks finishing
      pool.join()
      break

  print ''


if __name__ == '__main__':
  main()

